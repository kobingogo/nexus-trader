import os
import time
import json
from typing import Dict, Any, Tuple
from openai import OpenAI
from datetime import datetime
from app.services.market_data import MarketDataService


class DailyReviewService:
    @staticmethod
    def stream_review():
        """
        Stream the daily review generation (yields chunks).
        Format: NDJSON (one JSON object per line).
        
        Yields:
            str: JSON string with "type" and "content"
        """
        yield json.dumps({"type": "status", "content": "æ­£åœ¨è·å–å¸‚åœºæ•°æ®..."}) + "\n"
        
        try:
            sentiment = MarketDataService.get_market_sentiment()
            heatmap = MarketDataService.get_sector_heatmap()
            leaders = MarketDataService.get_leader_stocks()
            
            context = DailyReviewService._build_context(sentiment, heatmap, leaders)
            
            yield json.dumps({"type": "status", "content": "æ•°æ®è·å–å®Œæˆï¼Œæ­£åœ¨ç”Ÿæˆæ·±åº¦åˆ†æ..."}) + "\n"
            
            # Get LLM Client
            from app.services.llm_provider import LLMProviderManager
            manager = LLMProviderManager()
            client, model_name = manager.get_client()
            
            if not client:
                 yield json.dumps({"type": "error", "content": "æœªé…ç½® LLMï¼Œæ— æ³•æµå¼ç”Ÿæˆå¤ç›˜å†…å®¹ã€‚"}) + "\n"
                 return

            # Single Master Prompt for continuous streaming
            yield json.dumps({"type": "chunk", "content": f"# ğŸ“ˆ NEXUS æ·±åº¦å¤ç›˜ ({time.strftime('%Y-%m-%d')})\n\n"}) + "\n"
            
            master_prompt = f"""
ä½ ç°åœ¨æ˜¯ NEXUS AI äº¤æ˜“ç³»ç»Ÿã€‚è¯·åŸºäºä»¥ä¸‹ã€å¸‚åœºæ•°æ®ã€‘ï¼ŒæŒ‰é¡ºåºè¾“å‡ºæ·±åº¦å¤ç›˜æŠ¥å‘Šã€‚

ã€å¸‚åœºæ•°æ®ã€‘
{context}

ã€æŠ¥å‘Šè¦æ±‚ã€‘
è¯·è¾“å‡ºä»¥ä¸‹å››ä¸ªéƒ¨åˆ†ï¼Œä½¿ç”¨ Markdown æ ¼å¼ï¼Œæ ‡é¢˜å±‚çº§éœ€æ˜ç¡®ï¼Œæ¯ä¸ªéƒ¨åˆ†çº¦ 200-300 å­—ï¼š

1. ## ğŸ›ï¸ æœºæ„è§†è§’ (Institutional)
   åˆ†æåŸºæœ¬é¢ã€å®è§‚æµåŠ¨æ€§ã€ä¸»æµæ¿å—è¶‹åŠ¿åŠé£æ ¼åˆ‡æ¢ã€‚é£æ ¼éœ€ä¸“ä¸šã€ç†æ€§ã€‚
2. ## ğŸ“Š é‡åŒ–è§†è§’ (Quantitative)
   åˆ†ææ¶¨è·Œæ¯”ã€èµšé’±æ•ˆåº”ã€å¸‚åœºå¹¿åº¦ã€èµ„é‡‘æµå‘å¼‚å¸¸ã€‚é£æ ¼éœ€å®¢è§‚ã€æ•°æ®é©±åŠ¨ã€‚
3. ## âš¡ æ¸¸èµ„è§†è§’ (Hot Money)
   åˆ†æé¢˜æåšå¼ˆã€è¿æ¿é«˜åº¦ã€æƒ…ç»ªå‘¨æœŸï¼ˆè¿æ¿ã€ååŒ…ã€æ ¸æŒ‰é’®ç­‰ï¼‰ã€‚é£æ ¼éœ€çŠ€åˆ©ã€ä¸“ä¸šæ¸¸èµ„æœ¯è¯­ä¸°å¯Œã€‚
4. ## ğŸ é¦–å¸­å›é¡¾ (CIO Summary)
   æ±‡æ€»ä»¥ä¸Šè§†è§’ï¼Œç»™å‡ºã€å¸‚åœºå®šè°ƒã€‘ã€ã€æ ¸å¿ƒç­–ç•¥ã€‘ï¼ˆå»ºè®®ä»“ä½ï¼‰åŠã€æ˜æ—¥é‡ç‚¹ã€‘ã€‚é£æ ¼éœ€æƒå¨ã€æœæ–­ã€‚

è¯·ç›´æ¥å¼€å§‹è¾“å‡ºæŠ¥å‘Šå†…å®¹ï¼Œä¸è¦æœ‰ä»»ä½•å¤šä½™çš„å¼€åœºç™½æˆ–ç»“æŸè¯­ã€‚
"""

            try:
                stream = client.chat.completions.create(
                    model=model_name,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªé¡¶çº§é‡‘èåˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿å¤šç»´åº¦è§†è§’åˆ‡å…¥åˆ†æè‚¡å¸‚ã€‚"},
                        {"role": "user", "content": master_prompt}
                    ],
                    temperature=0.7,
                    stream=True
                )
                
                for chunk in stream:
                    if hasattr(chunk, 'choices') and chunk.choices and chunk.choices[0].delta.content:
                        c = chunk.choices[0].delta.content
                        yield json.dumps({"type": "chunk", "content": c}) + "\n"
                
                yield json.dumps({"type": "chunk", "content": "\n\n---\n*NEXUS AI Â· æ·±åº¦å¤ç›˜ç³»ç»Ÿ*" }) + "\n"
                yield json.dumps({"type": "done", "content": ""}) + "\n"
                
            except Exception as e:
                error_msg = f"(æµå¼è§£æè¿‡ç¨‹å‡ºé”™: {str(e)})"
                yield json.dumps({"type": "error", "content": error_msg}) + "\n"

        except Exception as e:
            yield json.dumps({"type": "error", "content": f"ç³»ç»Ÿæ•…éšœ: {str(e)}"}) + "\n"

    @staticmethod
    def _build_context(sentiment, heatmap, leaders) -> str:
        """Helper to build market context string for LLM."""
        top_sectors = heatmap[:5] if heatmap else []
        bottom_sectors = sorted(heatmap, key=lambda x: x.get("change_pct", 0))[:3] if heatmap else []
        top_leaders = leaders[:5] if leaders else []

        context_lines = [
            "## ä»Šæ—¥å¸‚åœºæ•°æ®æ‘˜è¦",
            f"- ä¸Šæ¶¨å®¶æ•°: {sentiment.get('up_count', 'N/A')}",
            f"- ä¸‹è·Œå®¶æ•°: {sentiment.get('down_count', 'N/A')}",
            f"- æ¶¨åœ: {sentiment.get('limit_up_count', 'N/A')}",
            f"- è·Œåœ: {sentiment.get('limit_down_count', 'N/A')}",
            f"- æ´»è·ƒåº¦: {sentiment.get('activity', 'N/A')}%",
            "",
            "### æœ€å¼ºæ¿å— TOP5:",
        ]
        for s in top_sectors:
            leader_info = f"(é¢†æ¶¨: {s.get('leader_name', 'N/A')})" if s.get('leader_name') else ""
            context_lines.append(f"- {s['name']}: {s['change_pct']:+.2f}% {leader_info}")
        
        context_lines.append("")
        context_lines.append("### æœ€å¼±æ¿å— TOP3:")
        for s in bottom_sectors:
            context_lines.append(f"- {s['name']}: {s['change_pct']:+.2f}%")

        context_lines.append("")
        context_lines.append("### äººæ°”é¾™å¤´ TOP5:")
        for l in top_leaders:
            context_lines.append(f"- {l['name']}({l['code']}): Â¥{l['price']} ({l['change_pct']:+.1f}%)")
            
        return "\n".join(context_lines)

    @staticmethod
    def generate_review() -> Dict[str, Any]:
        """
        Synchronous version for non-streaming consumers.
        """
        try:
            sentiment = MarketDataService.get_market_sentiment()
            heatmap = MarketDataService.get_sector_heatmap()
            leaders = MarketDataService.get_leader_stocks()
            
            context = DailyReviewService._build_context(sentiment, heatmap, leaders)

            from app.services.llm_provider import LLMProviderManager
            manager = LLMProviderManager()
            client, model_name = manager.get_client()

            if client and model_name:
                prompt = f"è¯·ä½œä¸º NEXUS AI é¦–å¸­åˆ†æå¸ˆï¼ŒåŸºäºä»¥ä¸‹æ•°æ®ç”Ÿæˆä»Šæ—¥å¤ç›˜æŠ¥å‘Šï¼š\n\n{context}\n\næŠ¥å‘Šéœ€åŒ…å«ï¼šæœºæ„è§†è§’ã€é‡åŒ–è§†è§’ã€æ¸¸èµ„è§†è§’å’Œ CIO æ€»ç»“ã€‚"
                resp = client.chat.completions.create(
                    model=model_name,
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªèµ„æ·±æŠ•èµ„åŠ©æ‰‹ã€‚"},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.7,
                )
                report = resp.choices[0].message.content.strip()
                return {
                    "report": report,
                    "generated_at": int(time.time()),
                    "data_source": "ai",
                }
            else:
                report = DailyReviewService._generate_template_report(
                    sentiment, heatmap[:5], sorted(heatmap, key=lambda x: x.get("change_pct", 0))[:3], leaders[:5]
                )
                return {
                    "report": report,
                    "generated_at": int(time.time()),
                    "data_source": "template",
                }

        except Exception as e:
            return {
                "report": f"# âš ï¸ å¤ç›˜ç”Ÿæˆå¤±è´¥\n\né”™è¯¯: {str(e)}",
                "generated_at": int(time.time()),
                "data_source": "error",
            }

    @staticmethod
    def _generate_template_report(sentiment, top_sectors, bottom_sectors, top_leaders) -> str:
        """Fallback template logic."""
        up, down = sentiment.get("up_count", 0), sentiment.get("down_count", 0)
        mood = "å¤šå¤´ä¸»å¯¼" if up > down else "ç©ºå¤´ä¸»å¯¼" if down > up else "éœ‡è¡å¹³è¡¡"
        
        return f"# ğŸ“‹ NEXUS æ¯æ—¥å¤ç›˜ (Template)\n\nå¸‚åœºæ°›å›´ï¼š{mood}ã€‚ä¸Šæ¶¨ {up}ï¼Œä¸‹è·Œ {down}ã€‚"
